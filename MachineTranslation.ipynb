{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIfdm48Hb6v38HtcqHR8IP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aishwarya138/Synapse-Learning-period/blob/NLP-week-4/MachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zxk4JszZ-ykG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import unicodedata\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
      ],
      "metadata": {
        "id": "u4HQVvFJ_qS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663cb184-c57a-4d53-ff59-e498b1386ba6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()  # removing extra space\n",
        "\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "Rftewkbx_qew"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentence = u\"May I borrow this @ book?\"\n",
        "spa_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "\n",
        "print(\"Before preprocessing\")\n",
        "print(\"English sentence : \", eng_sentence)\n",
        "print(\"Spanish sentence\", spa_sentence)\n",
        "print(\" \")\n",
        "print(\"After preprocessing\")\n",
        "print(\"English sentence : \", preprocess_sentence(eng_sentence))\n",
        "print(\"Spanish sentence\", preprocess_sentence(spa_sentence))"
      ],
      "metadata": {
        "id": "jqpdqkZa_qhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d10937-3625-4ef0-9cea-e71f65dc24d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before preprocessing\n",
            "English sentence :  May I borrow this @ book?\n",
            "Spanish sentence ¿Puedo tomar prestado este libro?\n",
            " \n",
            "After preprocessing\n",
            "English sentence :  <start> may i borrow this book ? <end>\n",
            "Spanish sentence <start> ¿ puedo tomar prestado este libro ? <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "eng, spa = create_dataset(path_to_file, None)\n",
        "print(eng[-1])\n",
        "print(spa[-1])"
      ],
      "metadata": {
        "id": "wp7s4-8F_qj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b3034a-160e-4112-aec0-23d90e4bee4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "EpjBdgUg_qml"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "1cY2UJNR_qpP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print(max_length_targ, max_length_inp)"
      ],
      "metadata": {
        "id": "DIh-CItN_qrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f031de35-50b3-4300-e084-c52567296a99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256    # for word embedding\n",
        "units = 1024    # dimensionality of the output space of RNN\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "id": "ymlhmuYP_qxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6329af-bc90-44bf-921e-0cce4870d944"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,  \n",
        "                                   return_state=True,  \n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "NfPGvNIaHF9y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "sZuDhzBuSftE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_layer = None):\n",
        "    super(DecoderWithAttention, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = attention_layer\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    x = self.embedding(x)\n",
        "    attention_weights = None\n",
        "    \n",
        "    if self.attention:\n",
        "      context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "metadata": {
        "id": "ULlUZoIy_q55"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "lr2OnQ4HHccx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def get_train_step_func():\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(inp, targ, enc_hidden, encoder, decoder):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape: \n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "    \n",
        "  return train_step"
      ],
      "metadata": {
        "id": "F1szQrLNHdCH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_seq2seq(epochs, attention):\n",
        "  encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention)\n",
        "  train_step_func = get_train_step_func()\n",
        "  training_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step_func(inp, targ, enc_hidden, encoder, decoder)\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "        \n",
        "\n",
        "    training_loss.append(total_loss / steps_per_epoch)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, training_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    \n",
        "  return encoder, decoder, training_loss"
      ],
      "metadata": {
        "id": "RCmR2HrMHdHg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "attention = None\n",
        "print(\"Running seq2seq model without attention\")\n",
        "encoder, decoder, training_loss = training_seq2seq(epochs, attention)\n",
        "tloss = training_loss"
      ],
      "metadata": {
        "id": "D8a2cpqsR65n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb3ea9f-253e-4ed2-82f4-c6eb31d2e666"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running seq2seq model without attention\n",
            "Epoch 1 Batch 0 Loss 4.5898\n",
            "Epoch 1 Batch 100 Loss 2.1447\n",
            "Epoch 1 Batch 200 Loss 1.9102\n",
            "Epoch 1 Batch 300 Loss 1.6986\n",
            "Epoch 1 Batch 400 Loss 1.6464\n",
            "Epoch 1 Loss 1.8993\n",
            "Time taken for 1 epoch 64.80535435676575 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4226\n",
            "Epoch 2 Batch 100 Loss 1.4372\n",
            "Epoch 2 Batch 200 Loss 1.3939\n",
            "Epoch 2 Batch 300 Loss 1.3053\n",
            "Epoch 2 Batch 400 Loss 1.3179\n",
            "Epoch 2 Loss 1.3690\n",
            "Time taken for 1 epoch 44.46726202964783 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1187\n",
            "Epoch 3 Batch 100 Loss 1.1175\n",
            "Epoch 3 Batch 200 Loss 1.1739\n",
            "Epoch 3 Batch 300 Loss 1.0470\n",
            "Epoch 3 Batch 400 Loss 1.0358\n",
            "Epoch 3 Loss 1.1076\n",
            "Time taken for 1 epoch 44.40150785446167 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8900\n",
            "Epoch 4 Batch 100 Loss 0.9261\n",
            "Epoch 4 Batch 200 Loss 0.9644\n",
            "Epoch 4 Batch 300 Loss 0.8785\n",
            "Epoch 4 Batch 400 Loss 0.9130\n",
            "Epoch 4 Loss 0.9019\n",
            "Time taken for 1 epoch 44.621570110321045 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7430\n",
            "Epoch 5 Batch 100 Loss 0.6954\n",
            "Epoch 5 Batch 200 Loss 0.7278\n",
            "Epoch 5 Batch 300 Loss 0.6946\n",
            "Epoch 5 Batch 400 Loss 0.7318\n",
            "Epoch 5 Loss 0.7346\n",
            "Time taken for 1 epoch 44.405765533447266 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5740\n",
            "Epoch 6 Batch 100 Loss 0.5294\n",
            "Epoch 6 Batch 200 Loss 0.6005\n",
            "Epoch 6 Batch 300 Loss 0.5753\n",
            "Epoch 6 Batch 400 Loss 0.6169\n",
            "Epoch 6 Loss 0.5957\n",
            "Time taken for 1 epoch 44.268646001815796 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4136\n",
            "Epoch 7 Batch 100 Loss 0.4285\n",
            "Epoch 7 Batch 200 Loss 0.4758\n",
            "Epoch 7 Batch 300 Loss 0.4557\n",
            "Epoch 7 Batch 400 Loss 0.4558\n",
            "Epoch 7 Loss 0.4717\n",
            "Time taken for 1 epoch 44.33197355270386 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.3817\n",
            "Epoch 8 Batch 100 Loss 0.3707\n",
            "Epoch 8 Batch 200 Loss 0.3519\n",
            "Epoch 8 Batch 300 Loss 0.3491\n",
            "Epoch 8 Batch 400 Loss 0.4350\n",
            "Epoch 8 Loss 0.3705\n",
            "Time taken for 1 epoch 44.30735969543457 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2793\n",
            "Epoch 9 Batch 100 Loss 0.2562\n",
            "Epoch 9 Batch 200 Loss 0.3226\n",
            "Epoch 9 Batch 300 Loss 0.2477\n",
            "Epoch 9 Batch 400 Loss 0.2929\n",
            "Epoch 9 Loss 0.2870\n",
            "Time taken for 1 epoch 44.26100730895996 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1814\n",
            "Epoch 10 Batch 100 Loss 0.2224\n",
            "Epoch 10 Batch 200 Loss 0.2195\n",
            "Epoch 10 Batch 300 Loss 0.2178\n",
            "Epoch 10 Batch 400 Loss 0.2639\n",
            "Epoch 10 Loss 0.2203\n",
            "Time taken for 1 epoch 44.29035782814026 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "attention = BahdanauAttention(units)\n",
        "print(\"Running seq2seq model with Bahdanau attention\")\n",
        "encoder_bah, decoder_bah, training_loss = training_seq2seq(epochs, attention)\n",
        "tloss = np.vstack((tloss, training_loss))"
      ],
      "metadata": {
        "id": "RxhC6xV8SpGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635c9104-0e6e-49a9-cd19-b0dcf78d8534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running seq2seq model with Bahdanau attention\n",
            "Epoch 1 Batch 0 Loss 4.6507\n",
            "Epoch 1 Batch 100 Loss 2.0514\n",
            "Epoch 1 Batch 200 Loss 1.5292\n",
            "Epoch 1 Batch 300 Loss 1.4601\n",
            "Epoch 1 Batch 400 Loss 1.2816\n",
            "Epoch 1 Loss 1.6856\n",
            "Time taken for 1 epoch 98.92546105384827 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9885\n",
            "Epoch 2 Batch 100 Loss 0.8711\n",
            "Epoch 2 Batch 200 Loss 0.7430\n",
            "Epoch 2 Batch 300 Loss 0.7838\n",
            "Epoch 2 Batch 400 Loss 0.8050\n",
            "Epoch 2 Loss 0.8118\n",
            "Time taken for 1 epoch 81.99607419967651 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5456\n",
            "Epoch 3 Batch 100 Loss 0.4241\n",
            "Epoch 3 Batch 200 Loss 0.4263\n",
            "Epoch 3 Batch 300 Loss 0.5504\n",
            "Epoch 3 Batch 400 Loss 0.4425\n",
            "Epoch 3 Loss 0.4657\n",
            "Time taken for 1 epoch 82.03529095649719 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2344\n",
            "Epoch 4 Batch 100 Loss 0.2591\n",
            "Epoch 4 Batch 200 Loss 0.2809\n",
            "Epoch 4 Batch 300 Loss 0.2318\n",
            "Epoch 4 Batch 400 Loss 0.2812\n",
            "Epoch 4 Loss 0.2800\n",
            "Time taken for 1 epoch 81.97464537620544 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1923\n",
            "Epoch 5 Batch 100 Loss 0.2037\n",
            "Epoch 5 Batch 200 Loss 0.1662\n",
            "Epoch 5 Batch 300 Loss 0.2097\n",
            "Epoch 5 Batch 400 Loss 0.1735\n",
            "Epoch 5 Loss 0.1814\n",
            "Time taken for 1 epoch 81.88835072517395 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1267\n",
            "Epoch 6 Batch 100 Loss 0.0962\n",
            "Epoch 6 Batch 200 Loss 0.0955\n",
            "Epoch 6 Batch 300 Loss 0.1215\n",
            "Epoch 6 Batch 400 Loss 0.1151\n",
            "Epoch 6 Loss 0.1269\n",
            "Time taken for 1 epoch 81.85804343223572 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0853\n",
            "Epoch 7 Batch 100 Loss 0.0721\n",
            "Epoch 7 Batch 200 Loss 0.0773\n",
            "Epoch 7 Batch 300 Loss 0.1183\n",
            "Epoch 7 Batch 400 Loss 0.1055\n",
            "Epoch 7 Loss 0.0974\n",
            "Time taken for 1 epoch 81.78463363647461 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0721\n",
            "Epoch 8 Batch 100 Loss 0.0858\n",
            "Epoch 8 Batch 200 Loss 0.0620\n",
            "Epoch 8 Batch 300 Loss 0.0734\n",
            "Epoch 8 Batch 400 Loss 0.0909\n",
            "Epoch 8 Loss 0.0801\n",
            "Time taken for 1 epoch 81.92500472068787 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0317\n",
            "Epoch 9 Batch 100 Loss 0.0488\n",
            "Epoch 9 Batch 200 Loss 0.0839\n",
            "Epoch 9 Batch 300 Loss 0.0737\n",
            "Epoch 9 Batch 400 Loss 0.0779\n",
            "Epoch 9 Loss 0.0710\n",
            "Time taken for 1 epoch 81.87820935249329 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0606\n",
            "Epoch 10 Batch 100 Loss 0.0400\n",
            "Epoch 10 Batch 200 Loss 0.0539\n",
            "Epoch 10 Batch 300 Loss 0.0544\n",
            "Epoch 10 Batch 400 Loss 0.0644\n",
            "Epoch 10 Loss 0.0646\n",
            "Time taken for 1 epoch 81.81167364120483 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, encoder, decoder):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "metadata": {
        "id": "7ULrBypRHdNL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence = translate(u'esta es mi vida.', encoder_bah, decoder_bah)\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "id": "6jamUEyvSwSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393b0459-1756-4880-afac-825200f48d2b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have performed machine translation from Spanish to English using Encoder Decoder model with Bahdanau Attention i.e Additive attention. The encoder generates a set of annotations from the input sentence and these annotations are fed to an alignment model together with the previous hidden encoder state. The alignment model uses this information to generate the attention scores. A softmax function is applied to the attention scores, effectively normalizing them into weight values in a range between 0 and 1. These weights together with the previously computed annotations are used to generate a context vector through a weighted sum of the annotations. The context vector is fed to the decoder together with the previous hidden decoder state and the previous utput, to compute the final output. These steps are repeated until the end of the sequence. \n",
        "I have used a GRU layer in the encoder to annotate the input sentenes.\n",
        "I have used teacher forcing for training where we pass the actual word to the Decoder at each time step. Then, calculate the gradient descent, apply it to the optimizer and backpropagate."
      ],
      "metadata": {
        "id": "F9c8vU4OBFVT"
      }
    }
  ]
}